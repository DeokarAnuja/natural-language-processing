{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Words, Phrases and concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy.blank() creates an object that contains processing pipeline.\n",
    "\n",
    "nlp also includes language-specific rules used for tokenizing the text into words and punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Create a blank English nlp object\n",
    "nlp = spacy.blank(\"en\")                     # de for German , es for Spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call nlp on a string, spaCy first tokenizes the text and creates a document object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"Sentence in English can be processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documents,Spans and Tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first token\n",
    "first_token = doc[0]\n",
    "\n",
    "# Print the first token's text\n",
    "print(first_token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Slice of document\n",
    "\n",
    "You can access document token by token\n",
    "\n",
    "i.e\n",
    "\n",
    "If we write doc[2:5]\n",
    "It will access from 2nd word till 4th word.5th word will not be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slice Sentence in English\n",
    "first_slice=doc[0:3]\n",
    "\n",
    "print(first_slice)\n",
    "\n",
    "\n",
    "#Slice English can be processed\n",
    "second_slice=doc[2:6]\n",
    "\n",
    "print(second_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token class has different attribute\n",
    "\n",
    "Attributes of token can be accessed using dot operator\n",
    "i.e if t is token then attributes can be accessed by t.attribute_name  \n",
    "\n",
    "Following are some of the attribute\n",
    "\n",
    "i &nbsp; &nbsp;&nbsp;                   index of token<br>\n",
    "ent_type &nbsp;&nbsp;                Named Entity type(int)<br>\n",
    "ent_type_  &nbsp;&nbsp;              Named Entity type(str)<br>\n",
    "lower_ &nbsp;&nbsp;&nbsp;                  Lowercase of token<br>\n",
    "is_alpha &nbsp;&nbsp;                Does the token consist of alphabetic characters?<br> \n",
    "is_digit &nbsp;&nbsp;                Does the token consist of digits?<br>\n",
    "is_lower &nbsp;&nbsp;                Is the token in lowercase? <br>\n",
    "is_upper &nbsp;&nbsp;                Is the token in uppercase?<br> \n",
    "is_title &nbsp;&nbsp;                Is the token in titlecase?<br>\n",
    "is_punct &nbsp;&nbsp;                Is the token punctuation?<br>\n",
    "is_space &nbsp;&nbsp;                Does the token consist of whitespace characters?<br> \n",
    "like_url &nbsp;&nbsp;                Does the token resemble a URL?<br>\n",
    "like_num &nbsp;&nbsp;                Does the token represent a number?<br>\n",
    "like_email &nbsp;&nbsp;              Does the token resemble an email address?<br>\n",
    "pos_ &nbsp;&nbsp;&nbsp;                   Coarse-grained part-of-speech from the Universal POS tag set.<br>\n",
    "\n",
    "To check list: https://spacy.io/api/token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=doc[5]\n",
    "\n",
    "print(f\"i \\t\\t\\t {token.i}\")\n",
    "print(f\"ent_type \\t\\t {token.ent_type}\")\n",
    "print(f\"lower \\t\\t\\t {token.lower_}\")\n",
    "print(f\"is_alpa \\t\\t {token.is_alpha}\")\n",
    "print(f\"is_digit \\t\\t {token.is_digit}\")\n",
    "print(f\"is_lower \\t\\t {token.is_lower}\")\n",
    "print(f\"is_title \\t\\t {token.is_title}\")\n",
    "print(f\"is_space \\t\\t {token.is_space}\")\n",
    "print(f\"pos_ \\t\\t\\t {token.pos_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(\"This is url https://spacy.io/api/token\")\n",
    "token=doc[3]\n",
    "token.like_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(\"This is url mymail@gmail.com\")\n",
    "token=doc[3]\n",
    "token.like_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(\"This is percent 44%\")\n",
    "token=doc[3]\n",
    "token.like_num\n",
    "#token.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Percentage from given Text\n",
    "\n",
    "Task:<br>\n",
    "   Iterate to access every token.<br>\n",
    "   Check if each token represents number<br>\n",
    "   If token represents number check if next token is % sign<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_list=[]\n",
    "for token in doc:\n",
    "    if token.like_num:\n",
    "        next_token=doc[token.i+1]\n",
    "        if next_token.text==\"%\":\n",
    "            percent_list.append(token.text)\n",
    "percent_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained Piplines\n",
    "\n",
    "spaCy provides a number of trained pipeline packages you can download using the spacy download command.<br>\n",
    "The spacy.load method loads a pipeline package by name and returns an nlp object.<br>\n",
    "The package provides the binary weights that enable spaCy to make predictions.<br>\n",
    "It also includes the vocabulary, meta information about the pipeline and the configuration file used to train it. <br>\n",
    "\n",
    "Trained English Piplines:<br>\n",
    "en_core_web_sm &nbsp;   small<br>\n",
    "en_core_web_md &nbsp;   medium<br>\n",
    "en_core_web_lg &nbsp;   large<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download large English Package \n",
    "#use en_core_web_sm for minimum time\n",
    "!python -m spacy download en_core_web_sm   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Part of speech \n",
    "\n",
    "Load Language Pipeline<br>\n",
    "Process text<br>\n",
    "Iterate to check each token<br>\n",
    "Print token and predicted part of speech.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pipeline\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#process text\n",
    "doc=nlp(\"I should ask my sister. She is really good at it.\")\n",
    "\n",
    "#Iterate over text\n",
    "for token in doc:\n",
    "    # predict part of speech\n",
    "    \"\"\"\n",
    "    pos_    Part of speech\n",
    "    dep_   Syntactic dependency relation.Predicted dependency label\n",
    "    head   Syntactic head token. You can also think of it as the parent token this word is attached to.\n",
    "    \"\"\"\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Something Additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of DEP n Meaning\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for label in nlp.get_pipe(\"parser\").labels:\n",
    "    print(label, \" -- \", spacy.explain(label))\n",
    "\n",
    "# get lsit of tags n Meaning\n",
    "for label in nlp.get_pipe(\"tagger\").labels:\n",
    "    print(label, \" -- \", spacy.explain(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='syntatic-dependency.png' width=\"1200\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Named Entities\n",
    "\n",
    "Named entities are \"real world objects\" that are assigned a name â€“ for example, a person, an organization or a country.<br>\n",
    "We can get predicted named entities using ents attribute of doc.<br>\n",
    "It returns an iterator of Span objects, so we can print the entity text and the entity label using the .label_ attribute<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(\"Shyam likes to play football\")\n",
    "\n",
    "for entities in doc.ents:\n",
    "    print(entities.text,entities.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing named entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "text = \"Aditi reads book everyday.She started reading Atomic habits on 5 Feb 2022. Book price is 79$\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get quick definitions of the most common tags and labels.\n",
    "\n",
    "spacy.explain method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"dobj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"NNP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule based Matching\n",
    "\n",
    "1.import the matcher from spacy.matcher.<br>\n",
    "2.Load a pipeline and create the nlp object.<br>\n",
    "3.Initialize matcher with the shared vocabulary nlp.vocab <br>\n",
    "4.The matcher.add method lets you add a pattern.<br>\n",
    " &nbsp;The first argument is a unique ID to identify which pattern was matched.<br>\n",
    " &nbsp;The second argument is a list of patterns.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a pipeline and create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [{\"TEXT\": \"python\"}]\n",
    "matcher.add(\"Python_Programming\", [pattern])\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"I am using python for nlp.\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "matches has 3 values\n",
    "match_id: hash value of the pattern name\n",
    "start: start index of matched span\n",
    "end: end index of matched span\n",
    "\"\"\"\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match_id, start, end in matches:\n",
    "    #get all matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching lexical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern for Smart Hackathon 2018\n",
    "pattern = [\n",
    "    \n",
    "    {\"LOWER\": \"smart\"},\n",
    "    {\"LOWER\": \"hackathon\"},\n",
    "    {\"IS_DIGIT\": True}\n",
    "  \n",
    "]\n",
    "#add pattern to matcher \n",
    "doc = nlp(\"Smart Hackathon 2020\")\n",
    "matcher.add(\"Hackathon\", [pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "#find match\n",
    "for match_id, start, end in matches:\n",
    "    #get all matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using operators and quantifiers\n",
    "\n",
    "Operators and quantifiers let you define how often a token should be matched.<br>\n",
    "They can be added using the \"OP\" key.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operators:\n",
    "\n",
    "An \"!\" negates the token, so it's matched 0 times.\n",
    "\n",
    "A \"?\" makes the token optional, and matches it 0 or 1 times.\n",
    "\n",
    "A \"+\" matches a token 1 or more times.\n",
    "\n",
    "And finally, an \"*\" matches 0 or more times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"LEMMA\": \"implement\"},           # LEMMA indicates base word\n",
    "    {\"POS\": \"DET\", \"OP\": \"?\"},        # ? is operator --> optional: match 0 or 1 times  for optional article(determiner)\n",
    "    {\"POS\": \"NOUN\"}                   # part of speech noun\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I have implemented miniproject\")\n",
    "matcher.add(\"implement_pattern\", [pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "#find match\n",
    "for match_id, start, end in matches:\n",
    "    #get all matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice making Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern --> Adjective + Noun +(Optional) Noun\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"Features of the app include a beautiful design, smart search, automatic \"\n",
    "    \"labels and optional voice responses.\"\n",
    ")\n",
    "\n",
    "# Write a pattern for adjective plus one or two nouns\n",
    "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\", \"OP\": \"?\"}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"ADJ_NOUN_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern --> download(base word)  + Proper noun\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"i downloaded Fortnite on my laptop and can't open the game at all. Help? \"\n",
    "    \"so when I was downloading Minecraft, I got the Windows version where it \"\n",
    "    \"is the '.zip' folder and I used the default program to unpack it... do \"\n",
    "    \"I also need to download Winzip?\"\n",
    ")\n",
    "\n",
    "# Write a pattern that matches a form of \"download\" plus proper noun\n",
    "pattern = [{\"LEMMA\": \"download\"}, {\"POS\": \"PROPN\"}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"DOWNLOAD_THINGS_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "abe380169102424e641c19689364de894815d06a51e17008597c4a1a65f4bfd4"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
