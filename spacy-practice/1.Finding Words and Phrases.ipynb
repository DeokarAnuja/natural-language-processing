{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Words, Phrases and concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy.blank() creates an object that contains processing pipeline.\n",
    "\n",
    "nlp also includes language-specific rules used for tokenizing the text into words and punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Create a blank English nlp object\n",
    "nlp = spacy.blank(\"en\")                     # de for German , es for Spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call nlp on a string, spaCy first tokenizes the text and creates a document object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"Sentence in English can be processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documents,Spans and Tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence\n"
     ]
    }
   ],
   "source": [
    "# Select the first token\n",
    "first_token = doc[0]\n",
    "\n",
    "# Print the first token's text\n",
    "print(first_token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Slice of document\n",
    "\n",
    "You can access document token by token\n",
    "\n",
    "i.e\n",
    "\n",
    "If we write doc[2:5]\n",
    "It will access from 2nd word till 4th word.5th word will not be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence in English\n",
      "English can be processed\n"
     ]
    }
   ],
   "source": [
    "#Slice Sentence in English\n",
    "first_slice=doc[0:3]\n",
    "\n",
    "print(first_slice)\n",
    "\n",
    "\n",
    "#Slice English can be processed\n",
    "second_slice=doc[2:6]\n",
    "\n",
    "print(second_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token class has different attribute\n",
    "\n",
    "Attributes of token can be accessed using dot operator\n",
    "i.e if t is token then attributes can be accessed by t.attribute_name  \n",
    "\n",
    "Following are some of the attribute\n",
    "\n",
    "i &nbsp; &nbsp;&nbsp;                   index of token<br>\n",
    "ent_type &nbsp;&nbsp;                Named Entity type(int)<br>\n",
    "ent_type_  &nbsp;&nbsp;              Named Entity type(str)<br>\n",
    "lower_ &nbsp;&nbsp;&nbsp;                  Lowercase of token<br>\n",
    "is_alpha &nbsp;&nbsp;                Does the token consist of alphabetic characters?<br> \n",
    "is_digit &nbsp;&nbsp;                Does the token consist of digits?<br>\n",
    "is_lower &nbsp;&nbsp;                Is the token in lowercase? <br>\n",
    "is_upper &nbsp;&nbsp;                Is the token in uppercase?<br> \n",
    "is_title &nbsp;&nbsp;                Is the token in titlecase?<br>\n",
    "is_punct &nbsp;&nbsp;                Is the token punctuation?<br>\n",
    "is_space &nbsp;&nbsp;                Does the token consist of whitespace characters?<br> \n",
    "like_url &nbsp;&nbsp;                Does the token resemble a URL?<br>\n",
    "like_num &nbsp;&nbsp;                Does the token represent a number?<br>\n",
    "like_email &nbsp;&nbsp;              Does the token resemble an email address?<br>\n",
    "pos_ &nbsp;&nbsp;&nbsp;                   Coarse-grained part-of-speech from the Universal POS tag set.<br>\n",
    "\n",
    "To check list: https://spacy.io/api/token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token=doc[2]\n",
    "\n",
    "token.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.ent_type_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'english'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.lower_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.is_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.is_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.is_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.is_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=nlp(\"This is url https://spacy.io/api/token\")\n",
    "token=doc[3]\n",
    "token.like_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=nlp(\"This is url mymail@gmail.com\")\n",
    "token=doc[3]\n",
    "token.like_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'44'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=nlp(\"This is url 44%\")\n",
    "token=doc[3]\n",
    "token.like_num\n",
    "#token.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Percentage from given Text\n",
    "\n",
    "Task:<br>\n",
    "   Iterate to access every token.<br>\n",
    "   Check if each token represents number<br>\n",
    "   If token represents number check if next token is % sign<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['44']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_list=[]\n",
    "for token in doc:\n",
    "    if token.like_num:\n",
    "        next_token=doc[token.i+1]\n",
    "        if next_token.text==\"%\":\n",
    "            percent_list.append(token.text)\n",
    "percent_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained Piplines\n",
    "\n",
    "spaCy provides a number of trained pipeline packages you can download using the spacy download command.<br>\n",
    "The spacy.load method loads a pipeline package by name and returns an nlp object.<br>\n",
    "The package provides the binary weights that enable spaCy to make predictions.<br>\n",
    "It also includes the vocabulary, meta information about the pipeline and the configuration file used to train it. <br>\n",
    "\n",
    "Trained English Piplines:<br>\n",
    "en_core_web_sm &nbsp;   small<br>\n",
    "en_core_web_md &nbsp;   medium<br>\n",
    "en_core_web_lg &nbsp;   large<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download large English Package \n",
    "#use en_core_web_sm for minimum time\n",
    "!python -m spacy download en_core_web_sm   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Part of speech \n",
    "\n",
    "Load Language Pipeline<br>\n",
    "Process text<br>\n",
    "Iterate to check each token<br>\n",
    "Print token and predicted part of speech.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pipeline\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#process text\n",
    "doc=nlp(\"I should ask my sister. She is really good at it.\")\n",
    "\n",
    "#Iterate over text\n",
    "for token in doc:\n",
    "    # predict part of speech\n",
    "    \"\"\"\n",
    "    pos_    Part of speech\n",
    "    dep_   Syntactic dependency relation.Predicted dependency label\n",
    "    head   Syntactic head token. You can also think of it as the parent token this word is attached to.\n",
    "    \"\"\"\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Something Additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of DEP n Meaning\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for label in nlp.get_pipe(\"parser\").labels:\n",
    "    print(label, \" -- \", spacy.explain(label))\n",
    "\n",
    "# get lsit of tags n Meaning\n",
    "for label in nlp.get_pipe(\"tagger\").labels:\n",
    "    print(label, \" -- \", spacy.explain(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='syntatic-dependency.png' width=\"1200\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Named Entities\n",
    "\n",
    "Named entities are \"real world objects\" that are assigned a name – for example, a person, an organization or a country.<br>\n",
    "We can get predicted named entities using ents attribute of doc.<br>\n",
    "It returns an iterator of Span objects, so we can print the entity text and the entity label using the .label_ attribute<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shyam PERSON\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"Shyam likes to play football\")\n",
    "\n",
    "for entities in doc.ents:\n",
    "    print(entities.text,entities.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing named entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Aditi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " reads book \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    everyday\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".She started reading Atomic habits on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    5 Feb 2022\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". Book price is \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    79$\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "text = \"Aditi reads book everyday.She started reading Atomic habits on 5 Feb 2022. Book price is 79$\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get quick definitions of the most common tags and labels.\n",
    "\n",
    "spacy.explain method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'direct object'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"dobj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noun, proper singular'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"NNP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule based Matching\n",
    "\n",
    "1.import the matcher from spacy.matcher.<br>\n",
    "2.Load a pipeline and create the nlp object.<br>\n",
    "3.Initialize matcher with the shared vocabulary nlp.vocab <br>\n",
    "4.The matcher.add method lets you add a pattern.<br>\n",
    " &nbsp;The first argument is a unique ID to identify which pattern was matched.<br>\n",
    " &nbsp;The second argument is a list of patterns.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a pipeline and create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [{\"TEXT\": \"python\"}]\n",
    "matcher.add(\"Python_Programming\", [pattern])\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"I am using python for nlp.\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5497254226871022622, 3, 4)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "matches has 3 values\n",
    "match_id: hash value of the pattern name\n",
    "start: start index of matched span\n",
    "end: end index of matched span\n",
    "\"\"\"\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    #get all matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching lexical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern for Smart Hackathon 2018\n",
    "pattern = [\n",
    "    \n",
    "    {\"LOWER\": \"smart\"},\n",
    "    {\"LOWER\": \"hackathon\"},\n",
    "    {\"IS_DIGIT\": True}\n",
    "  \n",
    "]\n",
    "#add pattern to matcher \n",
    "doc = nlp(\"Smart Hackathon 2020\")\n",
    "matcher.add(\"Hackathon\", [pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "#find match\n",
    "for match_id, start, end in matches:\n",
    "    #get all matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using operators and quantifiers\n",
    "\n",
    "Operators and quantifiers let you define how often a token should be matched.<br>\n",
    "They can be added using the \"OP\" key.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operators:\n",
    "\n",
    "An \"!\" negates the token, so it's matched 0 times.\n",
    "\n",
    "A \"?\" makes the token optional, and matches it 0 or 1 times.\n",
    "\n",
    "A \"+\" matches a token 1 or more times.\n",
    "\n",
    "And finally, an \"*\" matches 0 or more times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"LEMMA\": \"implement\"},           # LEMMA indicates base word\n",
    "    {\"POS\": \"DET\", \"OP\": \"?\"},        # ? is operator --> optional: match 0 or 1 times  for optional article(determiner)\n",
    "    {\"POS\": \"NOUN\"}                   # part of speech noun\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "implemented miniproject\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I have implemented miniproject\")\n",
    "matcher.add(\"implement_pattern\", [pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "#find match\n",
    "for match_id, start, end in matches:\n",
    "    #get all matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice making Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern --> Adjective + Noun +(Optional) Noun\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"Features of the app include a beautiful design, smart search, automatic \"\n",
    "    \"labels and optional voice responses.\"\n",
    ")\n",
    "\n",
    "# Write a pattern for adjective plus one or two nouns\n",
    "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\", \"OP\": \"?\"}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"ADJ_NOUN_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n",
      "Match found: downloaded Fortnite\n",
      "Match found: downloading Minecraft\n",
      "Match found: download Winzip\n"
     ]
    }
   ],
   "source": [
    "#pattern --> download(base word)  + Proper noun\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"i downloaded Fortnite on my laptop and can't open the game at all. Help? \"\n",
    "    \"so when I was downloading Minecraft, I got the Windows version where it \"\n",
    "    \"is the '.zip' folder and I used the default program to unpack it... do \"\n",
    "    \"I also need to download Winzip?\"\n",
    ")\n",
    "\n",
    "# Write a pattern that matches a form of \"download\" plus proper noun\n",
    "pattern = [{\"LEMMA\": \"download\"}, {\"POS\": \"PROPN\"}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"DOWNLOAD_THINGS_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "abe380169102424e641c19689364de894815d06a51e17008597c4a1a65f4bfd4"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
